# zookeeper 原理

## zookeeper 数据同步流程

​		在zookeeper 设计中，可以知道zookeeper是由三种集群角色来组成整个高性能的。客户端会随机连接到zookeeper集群中的一个节点，如果是读请求，就直接从当前节点中读取数据，如果是写请求，那么请求会被转发给leader提交事务，然后leader会广播事务，只要有超过半数节点写入成功，那么些请求就会被提交（类似2PC事务）。

从上面可以发现几个问题：

1. 集群中的leader是如何选举出来的？
2. leader 节点奔溃后，整个集群无法处理写请求，如何快速从其他节点里面选举出新的leader呢？
3. leader 节点和各个follower 节点的数据一致性如何保证的



### ZAB协议

​		ZAB（zookeeper atomic broadcast）协议是为分布式协调服务 zookeeper 专门设计的一种支持奔溃恢复的原子广播协议。在zookeeper 中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，zookeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。



#### zab 协议介绍

zab 协议包含两种基本模式，分别是

1. 奔溃恢复
2. 原子广播



​		当整个集群在启动时，或者当leader节点出现网络中断、奔溃等情况，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来之后，并且集群中有过半的机器和该leader节点完成数据同步后，ZAB协议机会退出恢复模式。

​		当集群中有过半的follower节点完成了和leader状态同步之后，那么整个集群就进入了消息广播模式。这个时候，在leader 节点正常工作时，启动一台新的服务器加入到集群，那这个服务器会直接进入数据恢复模式，和leader节点进行数据同步。同步完成后即可正常对外提供非事务请求的处理。



#### 消息广播的实现原理

消息广播的过程实际上是一个简化版本的二阶段提交过程。

1. leader接收到消息请求后，将消息赋予一个全局唯一的64位自增id（zxid），通过zxid的大小比较可以实现有序性
2. leader为每个follower准备了一个FIFO队列（通过TCP协议来实现，以实现全局有序这一个特点）将带有zxid的消息作为一个提案分发给所有follower
3. 当follower接受到提案，先把提案写到磁盘，写入成功后向leader回复一个ack
4. 当leader接受到合法数量的ack后，leader就会向这些follower发生commit命令，同时会在本地执行该消息
5. 当follower收到消息的commit 命令以后，会提交该消息



#### 奔溃恢复的实现原理



​		leader 因为各种问题失去了过半的follower节点的联系，那么就会进入到奔溃恢复模式，在奔溃恢复状态下，zab协议需要做两件事情：

1. 选举出新的leader
2. 数据同步



​		因为ZAB协议是一个简化的2PC协议，这种协议只需要集群中过半的节点响应提交即可。但是它无法处理leader服务器奔溃带来的数据不一致问题。

​		那么ZAB协议中的奔溃恢复需要保证，如果一个事务在一台机器上成功处理，那么这个事务应该在所有机器上处理成功。为了达到这个目的，zookeeper中会有哪些场景导致数据不一致性，以及针对这个场景，zab协议中的奔溃恢复应该怎么处理。

1. 已经被处理的消息不能丢：leader发送一个事务请求A，follower节点接收到请求，并返回ack，当leader发送commit命令前，leader奔溃这种情况下，ZAB协议需要事务A最终能在所有机器上成功提交
2. 被丢弃的消息不能再次出现：当leader接受到一个事务请求A之后，leader未转发给follower节点就奔溃，经过重新选举leader之后，事务A是被跳过的。原来的leader重启后变成了follower节点，它保留的事务A的状态跟集群的状态是不一致，需要删除。



​		ZAB 协议需要满足上面两种情况，就必须要设计一个leader选举算法：能够确保已经被leader提交的事务能够提交、同时丢弃已经被跳过的事务。

针对这个要求：

1. 如果leader选举算法能够保证新选举出来的leader服务器拥有集群中所有机器最高的编号（zxid）最大的事务请求，那么就可以保证这个新选举出来的leader一定具有已经提交的提案。
2. zxid 是有32位的epoch + 32位的消息计数器组成。这样设计的好处在于老的leader挂了以后重启，它不会被选举为leader。因此它的zxid肯定小于当前新的leader。当老的leader作为follower接入新的leader集群后，新的leader会让它将所有拥有旧的epoch号的未被commit的事务清除。





## leader选举原理



leader 选举存在两个阶段：

1. 启动时候的leader选举
2. 运行过程中，leader奔溃之后的选举



首先要知道选举过程设计到几个重要参数

- myid: 服务器对应的id，编号越大在算法中权重越大

- zxid: 事务id，值越大说明数据越新
- epoch: 投票次数，每一次投票会自增

选举状态

- LOOKING：竞选状态
- FOLLOWING：随从状态，同步leader状态，参与投票
- OBSERVING：观察状态，同步leader状态，不参与投票
- LEADING：领导者状态





### 服务器启动时的leader选举 

​		若进行lead选举，至少需要两台机器，假设有3台机器组成的集群。在server1启动时，其单独无法完成leader选举，当server2启动时，此时server1和server2可以互相通信，每台机器都试图找到leader，于是进入leader选举过程。选举过程如下：

1. 每个server发出一个投票。由于是初始状态，server1和server2都会将自己作为leader来进行投票，每次投票会包含锁推举的服务器的(myid,zxid,epoch)，此时server1的投票为(1,0,0)，server2的投票为(2,0,0)，然后各自将这个投票发给集群中的其它机器。
2. 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否本轮投票（epoch），是否来自LOOKING状态的服务器
3. 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下
   1. 优先比较epoch
   2. 再比较zxid，zxid最大的服务器有限作为leader
   3. 如果zxid相同，那就比较myid，myid最大的服务器作为leader
4. 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受相同的投票信息，对于server1、server2来说，都统计出集群中已经有两台机器接受了(2,0,0)的投票信息，此时便认为已经选出了leader
5. 改变服务器状态，一旦确定了leader，每个服务器就会更新自己的状态。

### 运行过程中的leader选举

​		当集群中的leader出现奔溃时，那么整个集群无法对外提供服务，而是进入新一轮的leader选举，服务器运行期间的leader选举和启动时期的leader选举基本过程是一致的。

1. 变更状态。leader奔溃后，余下的follower服务器都会讲自己的服务器状态变更为LOOKING。然后进入leader选举
2. 每个server会发出一个投票，在运行期间，每个服务器上的zxid可能不同，此时假定server1的zxid=123，server的zxid为122，在每一轮投票中，server1和server3都会投资及，产生投票(1,123,3)、(1,122,3)，然后各自将投票发送给集群中的所有机器。
3. 接受各自服务器的投票、处理投票、统计投票、改变服务器状态。（后续流程与启动选举相同）











