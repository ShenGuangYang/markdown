# 配置中心的源码分析

## 一个配置中心的功能

- 服务端的配置如何持久化保存
- 服务器端提供访问的api
- 数据变化之后如何通知客户端（push、pull）
- 客户端如何去获得远程服务的数据
- 安全性
- 客户端本地缓存

## Client 客户端使用

pom文件

```xml
<dependency>
  <groupId>com.alibaba.nacos</groupId>
  <artifactId>nacos-client</artifactId>
  <version>version</version>
</dependency>
```

java 代码

```java
public static void main(String[] args) throws NacosException {
  String serverAddr = "localhost:8848";
  String dataId = "example";
  String groupId = "DEFAULT_GROUP";
  Properties properties=new Properties();
  properties.put("serverAddr",serverAddr);

  ConfigService configService = NacosFactory.createConfigService(properties);
  String config = configService.getConfig(dataId, groupId, 3000);
  System.out.println(config);
}
```

## 源码分析

​		首先从 `ConfigService configService = NacosFactory.createConfigService(properties);` 作为的源码的入口。

### NacosFactory.createConfigService()

```java
public static ConfigService createConfigService(Properties properties) throws NacosException {
  return ConfigFactory.createConfigService(properties);
}
```

### ConfigFactory.createConfigService()

```java
public static ConfigService createConfigService(Properties properties) throws NacosException {
  try {
    // 反射了NacosConfigService类
    Class<?> driverImplClass = Class.forName("com.alibaba.nacos.client.config.NacosConfigService");
    // 获取构造方法
    Constructor constructor = driverImplClass.getConstructor(Properties.class);
    // 实例化构造方法
    ConfigService vendorImpl = (ConfigService) constructor.newInstance(properties);
    return vendorImpl;
  } catch (Throwable e) {
    throw new NacosException(NacosException.CLIENT_INVALID_PARAM, e);
  }
}
```

### NacosConfigService 构造

```java
public NacosConfigService(Properties properties) throws NacosException {
  // 初始化配置编码
  String encodeTmp = properties.getProperty(PropertyKeyConst.ENCODE);
  if (StringUtils.isBlank(encodeTmp)) {
    encode = Constants.ENCODE;
  } else {
    encode = encodeTmp.trim();
  }
  // 初始化命名空间
  initNamespace(properties);
  // 监控（不做具体分析）
  agent = new MetricsHttpAgent(new ServerHttpAgent(properties));
  agent.start();
  // 具体工作实现
  worker = new ClientWorker(agent, configFilterChainManager, properties);
}
```

### ClientWorker 构造

```java
public ClientWorker(final HttpAgent agent, final ConfigFilterChainManager configFilterChainManager, final Properties properties) {
  this.agent = agent;
  this.configFilterChainManager = configFilterChainManager;
	// 初始化timeout参数
  init(properties);
  // 创建一个定时任务线程池（异步处理）
  executor = Executors.newScheduledThreadPool(1, new ThreadFactory() {
    @Override
    public Thread newThread(Runnable r) {
      Thread t = new Thread(r);
      t.setName("com.alibaba.nacos.client.Worker." + agent.getName());
      t.setDaemon(true);
      return t;
    }
  });
	// 创建一个定时任务线程池（用来做长轮询使用）
  executorService = Executors.newScheduledThreadPool(Runtime.getRuntime().availableProcessors(), new ThreadFactory() {
    @Override
    public Thread newThread(Runnable r) {
      Thread t = new Thread(r);
      t.setName("com.alibaba.nacos.client.Worker.longPolling." + agent.getName());
      t.setDaemon(true);
      return t;
    }
  });
  // 创建一个带有延时的定时任务（检查配置内容）
  executor.scheduleWithFixedDelay(new Runnable() {
    @Override
    public void run() {
      try {
        // 检查本地配置
        checkConfigInfo();
      } catch (Throwable e) {
        LOGGER.error("[" + agent.getName() + "] [sub-check] rotate check error", e);
      }
    }
  }, 1L, 10L, TimeUnit.MILLISECONDS);
}
```

### ClientWorker.checkConfigInfo()

```java
public void checkConfigInfo() {
  // 因为一个数据缓存可能很大
  // 所以需要分任务处理
  int listenerSize = cacheMap.get().size();
  // 向上取整为批数
  int longingTaskCount = (int) Math.ceil(listenerSize / ParamUtil.getPerTaskConfigSize());
  if (longingTaskCount > currentLongingTaskCount) {
    for (int i = (int) currentLongingTaskCount; i < longingTaskCount; i++) {
      // 要判断任务是否在执行 这块需要好好想想。 任务列表现在是无序的。变化过程可能有问题
      executorService.execute(new LongPollingRunnable(i));
    }
    currentLongingTaskCount = longingTaskCount;
  }
}
```

### LongPollingRunnable.run()

```java
public void run() {

  List<CacheData> cacheDatas = new ArrayList<CacheData>();
  List<String> inInitializingCacheList = new ArrayList<String>();
  try {
    // check failover config
    // 检查本地配置
    for (CacheData cacheData : cacheMap.get().values()) {
      if (cacheData.getTaskId() == taskId) {
        cacheDatas.add(cacheData);
        try {
          // 检查本地配置
          checkLocalConfig(cacheData);
          if (cacheData.isUseLocalConfigInfo()) {
            cacheData.checkListenerMd5();
          }
        } catch (Exception e) {
          LOGGER.error("get local config info error", e);
        }
      }
    }

    // check server config
    // 检查服务端配置
    List<String> changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList);

    for (String groupKey : changedGroupKeys) {
      String[] key = GroupKey.parseKey(groupKey);
      String dataId = key[0];
      String group = key[1];
      String tenant = null;
      if (key.length == 3) {
        tenant = key[2];
      }
      try {
        // 获得服务端配置内容
        String content = getServerConfig(dataId, group, tenant, 3000L);
        CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant));
        cache.setContent(content);
        LOGGER.info("[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}",
                    agent.getName(), dataId, group, tenant, cache.getMd5(),
                    ContentUtils.truncateContent(content));
      } catch (NacosException ioe) {
        String message = String.format(
          "[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s",
          agent.getName(), dataId, group, tenant);
        LOGGER.error(message, ioe);
      }
    }
    // 对比服务端与本地缓存的配置信息
    for (CacheData cacheData : cacheDatas) {
      if (!cacheData.isInitializing() || inInitializingCacheList
          .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) {
        cacheData.checkListenerMd5();
        cacheData.setInitializing(false);
      }
    }
    inInitializingCacheList.clear();
		// 开启长轮询监听
    executorService.execute(this);

  } catch (Throwable e) {

    // If the rotation training task is abnormal, the next execution time of the task will be punished
    LOGGER.error("longPolling error : ", e);
    executorService.schedule(this, taskPenaltyTime, TimeUnit.MILLISECONDS);
  }
}
```

### ClientWorker.checkLocalConfig()

```java
private void checkLocalConfig(CacheData cacheData) {
  final String dataId = cacheData.dataId;
  final String group = cacheData.group;
  final String tenant = cacheData.tenant;
  // 本地缓存文件路径
  File path = LocalConfigInfoProcessor.getFailoverFile(agent.getName(), dataId, group, tenant);

  // 没有 -> 有
  if (!cacheData.isUseLocalConfigInfo() && path.exists()) {
    String content = LocalConfigInfoProcessor.getFailover(agent.getName(), dataId, group, tenant);
    String md5 = MD5.getInstance().getMD5String(content);
    cacheData.setUseLocalConfigInfo(true);
    cacheData.setLocalConfigInfoVersion(path.lastModified());
    cacheData.setContent(content);

    LOGGER.warn("[{}] [failover-change] failover file created. dataId={}, group={}, tenant={}, md5={}, content={}",
                agent.getName(), dataId, group, tenant, md5, ContentUtils.truncateContent(content));
    return;
  }

  // 有 -> 没有。不通知业务监听器，从server拿到配置后通知。
  if (cacheData.isUseLocalConfigInfo() && !path.exists()) {
    cacheData.setUseLocalConfigInfo(false);
    LOGGER.warn("[{}] [failover-change] failover file deleted. dataId={}, group={}, tenant={}", agent.getName(),
                dataId, group, tenant);
    return;
  }

  // 有变更
  if (cacheData.isUseLocalConfigInfo() && path.exists()
      && cacheData.getLocalConfigInfoVersion() != path.lastModified()) {
    String content = LocalConfigInfoProcessor.getFailover(agent.getName(), dataId, group, tenant);
    String md5 = MD5.getInstance().getMD5String(content);
    cacheData.setUseLocalConfigInfo(true);
    cacheData.setLocalConfigInfoVersion(path.lastModified());
    cacheData.setContent(content);
    LOGGER.warn("[{}] [failover-change] failover file changed. dataId={}, group={}, tenant={}, md5={}, content={}",
                agent.getName(), dataId, group, tenant, md5, ContentUtils.truncateContent(content));
  }
}
```

### ClientWorker.checkUpdateDataIds()

```java
/**
 * 从Server获取值变化了的DataID列表。返回的对象里只有dataId和group是有效的。 保证不返回NULL。
 */
List<String> checkUpdateDataIds(List<CacheData> cacheDatas, List<String> inInitializingCacheList) throws IOException {
  StringBuilder sb = new StringBuilder();
  for (CacheData cacheData : cacheDatas) {
    if (!cacheData.isUseLocalConfigInfo()) {
      sb.append(cacheData.dataId).append(WORD_SEPARATOR);
      sb.append(cacheData.group).append(WORD_SEPARATOR);
      if (StringUtils.isBlank(cacheData.tenant)) {
        sb.append(cacheData.getMd5()).append(LINE_SEPARATOR);
      } else {
        sb.append(cacheData.getMd5()).append(WORD_SEPARATOR);
        sb.append(cacheData.getTenant()).append(LINE_SEPARATOR);
      }
      if (cacheData.isInitializing()) {
        // cacheData 首次出现在cacheMap中&首次check更新
        inInitializingCacheList
          .add(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant));
      }
    }
  }
  boolean isInitializingCacheList = !inInitializingCacheList.isEmpty();
  return checkUpdateConfigStr(sb.toString(), isInitializingCacheList);
}
```

### ClientWorker.checkUpdateConfigStr()

```java
/**
 * 从Server获取值变化了的DataID列表。返回的对象里只有dataId和group是有效的。 保证不返回NULL。
 */
List<String> checkUpdateConfigStr(String probeUpdateString, boolean isInitializingCacheList) throws IOException {

  List<String> params = Arrays.asList(Constants.PROBE_MODIFY_REQUEST, probeUpdateString);

  List<String> headers = new ArrayList<String>(2);
  headers.add("Long-Pulling-Timeout");
  headers.add("" + timeout);

  // told server do not hang me up if new initializing cacheData added in
  if (isInitializingCacheList) {
    headers.add("Long-Pulling-Timeout-No-Hangup");
    headers.add("true");
  }

  if (StringUtils.isBlank(probeUpdateString)) {
    return Collections.emptyList();
  }

  try {
    // 发起一个请求监听
    HttpResult result = agent.httpPost(Constants.CONFIG_CONTROLLER_PATH + "/listener", headers, params, agent.getEncode(), timeout);

    if (HttpURLConnection.HTTP_OK == result.code) {
      setHealthServer(true);
      // 从HTTP响应拿到变化的groupKey。保证不返回NULL。
      return parseUpdateDataIdResponse(result.content);
    } else {
      setHealthServer(false);
      LOGGER.error("[{}] [check-update] get changed dataId error, code: {}", agent.getName(), result.code);
    }
  } catch (IOException e) {
    setHealthServer(false);
    LOGGER.error("[" + agent.getName() + "] [check-update] get changed dataId exception", e);
    throw e;
  }
  return Collections.emptyList();
}
```

### ClientWorker.getServerConfig()

```java
public String getServerConfig(String dataId, String group, String tenant, long readTimeout)
  throws NacosException {
  if (StringUtils.isBlank(group)) {
    group = Constants.DEFAULT_GROUP;
  }

  HttpResult result = null;
  try {
    List<String> params = null;
    if (StringUtils.isBlank(tenant)) {
      params = Arrays.asList("dataId", dataId, "group", group);
    } else {
      params = Arrays.asList("dataId", dataId, "group", group, "tenant", tenant);
    }
    // 发起http 请求从服务端获取最新数据
    result = agent.httpGet(Constants.CONFIG_CONTROLLER_PATH, null, params, agent.getEncode(), readTimeout);
  } catch (IOException e) {
    String message = String.format(
      "[%s] [sub-server] get server config exception, dataId=%s, group=%s, tenant=%s", agent.getName(),
      dataId, group, tenant);
    LOGGER.error(message, e);
    throw new NacosException(NacosException.SERVER_ERROR, e);
  }
  // 根据返回结果来进行判断
  switch (result.code) {
    case HttpURLConnection.HTTP_OK:
      LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, result.content);
      return result.content;
    case HttpURLConnection.HTTP_NOT_FOUND:
      LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, null);
      return null;
    case HttpURLConnection.HTTP_CONFLICT: {
      LOGGER.error(
        "[{}] [sub-server-error] get server config being modified concurrently, dataId={}, group={}, "
        + "tenant={}", agent.getName(), dataId, group, tenant);
      throw new NacosException(NacosException.CONFLICT,
                               "data being modified, dataId=" + dataId + ",group=" + group + ",tenant=" + tenant);
    }
    case HttpURLConnection.HTTP_FORBIDDEN: {
      LOGGER.error("[{}] [sub-server-error] no right, dataId={}, group={}, tenant={}", agent.getName(), dataId,
                   group, tenant);
      throw new NacosException(result.code, result.content);
    }
    default: {
      LOGGER.error("[{}] [sub-server-error]  dataId={}, group={}, tenant={}, code={}", agent.getName(), dataId,
                   group, tenant, result.code);
      throw new NacosException(result.code,
                               "http error, code=" + result.code + ",dataId=" + dataId + ",group=" + group + ",tenant=" + tenant);
    }
  }
}
```



### 长轮询监听请求服务端分析

长轮询监听的请求服务端接口是如下代码：

```java
HttpResult result = agent.httpPost(Constants.CONFIG_CONTROLLER_PATH + "/listener", headers, params, agent.getEncode(), timeout);
```

通过请求服务的url请求路径`/v1/cs/configs/listener` ，从服务端找到对应的执行类。

#### ConfigController.listener()

```java
@RequestMapping(value = "/listener", method = RequestMethod.POST)
public void listener(HttpServletRequest request, HttpServletResponse response)
  throws ServletException, IOException {
  request.setAttribute("org.apache.catalina.ASYNC_SUPPORTED", true);
  String probeModify = request.getParameter("Listening-Configs");
  if (StringUtils.isBlank(probeModify)) {
    throw new IllegalArgumentException("invalid probeModify");
  }

  probeModify = URLDecoder.decode(probeModify, Constants.ENCODE);

  Map<String, String> clientMd5Map;
  try {
    clientMd5Map = MD5Util.getClientMd5Map(probeModify);
  } catch (Throwable e) {
    throw new IllegalArgumentException("invalid probeModify");
  }

  // do long-polling
  inner.doPollingConfig(request, response, clientMd5Map, probeModify.length());
}
```



#### ConfigServletInner.doPollingConfig()

```java
public String doPollingConfig(HttpServletRequest request, HttpServletResponse response,
                              Map<String, String> clientMd5Map, int probeRequestSize)
  throws IOException, ServletException {

  // 长轮询
  if (LongPollingService.isSupportLongPolling(request)) {
    longPollingService.addLongPollingClient(request, response, clientMd5Map, probeRequestSize);
    return HttpServletResponse.SC_OK + "";
  }

  // else 兼容短轮询逻辑
  List<String> changedGroups = MD5Util.compareMd5(request, response, clientMd5Map);

  // 兼容短轮询result
  String oldResult = MD5Util.compareMd5OldResult(changedGroups);
  String newResult = MD5Util.compareMd5ResultString(changedGroups);

  String version = request.getHeader(Constants.CLIENT_VERSION_HEADER);
  if (version == null) {
    version = "2.0.0";
  }
  int versionNum = Protocol.getVersionNumber(version);

  /**
   * 2.0.4版本以前, 返回值放入header中
   */
  if (versionNum < START_LONGPOLLING_VERSION_NUM) {
    response.addHeader(Constants.PROBE_MODIFY_RESPONSE, oldResult);
    response.addHeader(Constants.PROBE_MODIFY_RESPONSE_NEW, newResult);
  } else {
    request.setAttribute("content", newResult);
  }

  // 禁用缓存
  response.setHeader("Pragma", "no-cache");
  response.setDateHeader("Expires", 0);
  response.setHeader("Cache-Control", "no-cache,no-store");
  response.setStatus(HttpServletResponse.SC_OK);
  return HttpServletResponse.SC_OK + "";
}
```

#### ConfigServletInner.addLongPollingClient()

主要的逻辑

- 获得客户端传递过来的超时时间，并且进行本地计算，提前500ms返回
- 根据客户端请求的md5和服务端对应的内容进行对比，如果不一致直接通过 `generateResponse` 返回
- 如果配置文件没有发生变化，则通过定时任务执行 `ClientLongPolling` 

```java
public void addLongPollingClient(HttpServletRequest req, HttpServletResponse rsp, Map<String, String> clientMd5Map,
                                 int probeRequestSize) {

  String str = req.getHeader(LongPollingService.LONG_POLLING_HEADER);
  String noHangUpFlag = req.getHeader(LongPollingService.LONG_POLLING_NO_HANG_UP_HEADER);
  String appName = req.getHeader(RequestUtil.CLIENT_APPNAME_HEADER);
  String tag = req.getHeader("Vipserver-Tag");
  int delayTime = SwitchService.getSwitchInteger(SwitchService.FIXED_DELAY_TIME, 500);
  /**
   * 提前500ms返回响应，为避免客户端超时 @qiaoyi.dingqy 2013.10.22改动  add delay time for LoadBalance
   */
  long timeout = Math.max(10000, Long.parseLong(str) - delayTime);
  if (isFixedPolling()) {
    timeout = Math.max(10000, getFixedPollingInterval());
    // do nothing but set fix polling timeout
  } else {
    long start = System.currentTimeMillis();
    List<String> changedGroups = MD5Util.compareMd5(req, rsp, clientMd5Map);
    if (changedGroups.size() > 0) {
      generateResponse(req, rsp, changedGroups);
      LogUtil.clientLog.info("{}|{}|{}|{}|{}|{}|{}",
                             System.currentTimeMillis() - start, "instant", RequestUtil.getRemoteIp(req), "polling",
                             clientMd5Map.size(), probeRequestSize, changedGroups.size());
      return;
    } else if (noHangUpFlag != null && noHangUpFlag.equalsIgnoreCase(TRUE_STR)) {
      LogUtil.clientLog.info("{}|{}|{}|{}|{}|{}|{}", System.currentTimeMillis() - start, "nohangup",
                             RequestUtil.getRemoteIp(req), "polling", clientMd5Map.size(), probeRequestSize,
                             changedGroups.size());
      return;
    }
  }
  String ip = RequestUtil.getRemoteIp(req);
  // 一定要由HTTP线程调用，否则离开后容器会立即发送响应
  final AsyncContext asyncContext = req.startAsync();
  // AsyncContext.setTimeout()的超时时间不准，所以只能自己控制
  asyncContext.setTimeout(0L);
	// 调用客户端长轮询线程
  scheduler.execute(
    new ClientLongPolling(asyncContext, clientMd5Map, ip, probeRequestSize, timeout, appName, tag));
}
```



#### LongPollingService.generateResponse()

```java
void generateResponse(HttpServletRequest request, HttpServletResponse response, List<String> changedGroups) {
  if (null == changedGroups) {
    return;
  }

  try {
    String respString = MD5Util.compareMd5ResultString(changedGroups);
    // 禁用缓存
    response.setHeader("Pragma", "no-cache");
    response.setDateHeader("Expires", 0);
    response.setHeader("Cache-Control", "no-cache,no-store");
    response.setStatus(HttpServletResponse.SC_OK);
    response.getWriter().println(respString);
  } catch (Exception se) {
    pullLog.error(se.toString(), se);
  }
}
```

#### ClientLongPolling.run()

```java
public void run() {
  asyncTimeoutFuture = scheduler.schedule(new Runnable() {
    @Override
    public void run() {
      try {
        getRetainIps().put(ClientLongPolling.this.ip, System.currentTimeMillis());
        /**
         * 删除订阅关系
         */
        allSubs.remove(ClientLongPolling.this);

        if (isFixedPolling()) {
          LogUtil.clientLog.info("{}|{}|{}|{}|{}|{}",
                                 (System.currentTimeMillis() - createTime),
                                 "fix", RequestUtil.getRemoteIp((HttpServletRequest)asyncContext.getRequest()),
                                 "polling",
                                 clientMd5Map.size(), probeRequestSize);
          List<String> changedGroups = MD5Util.compareMd5(
            (HttpServletRequest)asyncContext.getRequest(),
            (HttpServletResponse)asyncContext.getResponse(), clientMd5Map);
          if (changedGroups.size() > 0) {
            sendResponse(changedGroups);
          } else {
            sendResponse(null);
          }
        } else {
          LogUtil.clientLog.info("{}|{}|{}|{}|{}|{}",
                                 (System.currentTimeMillis() - createTime),
                                 "timeout", RequestUtil.getRemoteIp((HttpServletRequest)asyncContext.getRequest()),
                                 "polling",
                                 clientMd5Map.size(), probeRequestSize);
          sendResponse(null);
        }
      } catch (Throwable t) {
        LogUtil.defaultLog.error("long polling error:" + t.getMessage(), t.getCause());
      }

    }

  }, timeoutTime, TimeUnit.MILLISECONDS);

  allSubs.add(this);
}
```

`allSubs` 是一个队列，有加入队列的逻辑，那肯定有处理队列的逻辑。查看该类，继承了 `AbstractEventListener` 类，是用来进行事件监听的操作，具体实现在 `onEvent()` 。



### LongPollingService.onEvent()

```java
public void onEvent(Event event) {
  if (isFixedPolling()) {
    // ignore
  } else {
    // 判断是否LocalDataChangeEvent
    if (event instanceof LocalDataChangeEvent) {
      LocalDataChangeEvent evt = (LocalDataChangeEvent)event;
      scheduler.execute(new DataChangeTask(evt.groupKey, evt.isBeta, evt.betaIps));
    }
  }
}
```

### DataChangeTask.run()

```java
public void run() {
  try {
    ConfigService.getContentBetaMd5(groupKey);
    for (Iterator<ClientLongPolling> iter = allSubs.iterator(); iter.hasNext(); ) {
      ClientLongPolling clientSub = iter.next();
      if (clientSub.clientMd5Map.containsKey(groupKey)) {
        // 如果beta发布且不在beta列表直接跳过
        if (isBeta && !betaIps.contains(clientSub.ip)) {
          continue;
        }

        // 如果tag发布且不在tag列表直接跳过
        if (StringUtils.isNotBlank(tag) && !tag.equals(clientSub.tag)) {
          continue;
        }

        getRetainIps().put(clientSub.ip, System.currentTimeMillis());
        iter.remove(); // 删除订阅关系
        LogUtil.clientLog.info("{}|{}|{}|{}|{}|{}|{}",
                               (System.currentTimeMillis() - changeTime),
                               "in-advance",
                               RequestUtil.getRemoteIp((HttpServletRequest)clientSub.asyncContext.getRequest()),
                               "polling",
                               clientSub.clientMd5Map.size(), clientSub.probeRequestSize, groupKey);
        clientSub.sendResponse(Arrays.asList(groupKey));
      }
    }
  } catch (Throwable t) {
    LogUtil.defaultLog.error("data change error:" + t.getMessage(), t.getCause());
  }
}
```

那么有一个问题，数据变化之后是如何触发事件的呢？简单来想，那肯定是在数据有修改的时候就会触发监听，那在看服务接受数据的ConfigController 类



### ConfigController.publishConfig()

```java
@RequestMapping(method = RequestMethod.POST)
@ResponseBody
public Boolean publishConfig(.....)
  throws NacosException {
  final String srcIp = RequestUtil.getRemoteIp(request);
  String requestIpApp = RequestUtil.getAppName(request);

  Map<String, Object> configAdvanceInfo = new HashMap<String, Object>(10);
  // 省略部分代码
  final Timestamp time = TimeUtils.getCurrentTime();
  String betaIps = request.getHeader("betaIps");
  ConfigInfo configInfo = new ConfigInfo(dataId, group, tenant, appName, content);
  if (StringUtils.isBlank(betaIps)) {
    if (StringUtils.isBlank(tag)) {
      persistService.insertOrUpdate(srcIp, srcUser, configInfo, time, configAdvanceInfo, false);
      EventDispatcher.fireEvent(new ConfigDataChangeEvent(false, dataId, group, tenant, time.getTime()));
    } else {
      persistService.insertOrUpdateTag(configInfo, tag, srcIp, srcUser, time, false);
      EventDispatcher.fireEvent(new ConfigDataChangeEvent(false, dataId, group, tenant, tag, time.getTime()));
    }
  } else { // beta publish
    persistService.insertOrUpdateBeta(configInfo, betaIps, srcIp, srcUser, time, false);
    EventDispatcher.fireEvent(new ConfigDataChangeEvent(true, dataId, group, tenant, time.getTime()));
  }
  ConfigTraceService.logPersistenceEvent(dataId, group, tenant, requestIpApp, time.getTime(),
                                         LOCAL_IP, ConfigTraceService.PERSISTENCE_EVENT_PUB, content);

  return true;
}
```



​		找到配置变更的位置， 发现数据持久化之后，会通过EventDispatcher进行事件发布 `EventDispatcher.fireEvent()` 但是这个事件似乎不是我们所关心的时间，原因是这里发布的事件是 `ConfigDataChangeEvent` , 而 `LongPollingService` 感兴趣的事件是 `LocalDataChangeEvent`

​		从Config模块下找到一个DumpService类，它会定时把变更的数据写到磁盘上，DumpService在spring启动之后，会调用init方法，启动几个定时任务。

### DumpService.init()

```java
@PostConstruct
public void init() {
  LogUtil.defaultLog.warn("DumpService start");
  DumpProcessor processor = new DumpProcessor(this);
  DumpAllProcessor dumpAllProcessor = new DumpAllProcessor(this);
  DumpAllBetaProcessor dumpAllBetaProcessor = new DumpAllBetaProcessor(this);
  DumpAllTagProcessor dumpAllTagProcessor = new DumpAllTagProcessor(this);

  dumpTaskMgr = new TaskManager(
    "com.alibaba.nacos.server.DumpTaskManager");
  dumpTaskMgr.setDefaultTaskProcessor(processor);

  dumpAllTaskMgr = new TaskManager(
    "com.alibaba.nacos.server.DumpAllTaskManager");
  dumpAllTaskMgr.setDefaultTaskProcessor(dumpAllProcessor);

  Runnable dumpAll = new Runnable() {
    @Override
    public void run() {
      dumpAllTaskMgr.addTask(DumpAllTask.TASK_ID, new DumpAllTask());
    }
  };

  Runnable dumpAllBeta = new Runnable() {
    @Override
    public void run() {
      dumpAllTaskMgr.addTask(DumpAllBetaTask.TASK_ID, new DumpAllBetaTask());
    }
  };

  Runnable clearConfigHistory = new Runnable() {
    @Override
    public void run() {
      log.warn("clearConfigHistory start");
      if (ServerListService.isFirstIp()) {
        try {
          Timestamp startTime = getBeforeStamp(TimeUtils.getCurrentTime(), 24 * getRetentionDays());
          int totalCount = persistService.findConfigHistoryCountByTime(startTime);
          if (totalCount > 0) {
            int pageSize = 1000;
            int removeTime = (totalCount + pageSize - 1) / pageSize;
            log.warn("clearConfigHistory, getBeforeStamp:{}, totalCount:{}, pageSize:{}, removeTime:{}",
                     new Object[] {startTime, totalCount, pageSize, removeTime});
            while (removeTime > 0) {
              // 分页删除，以免批量太大报错
              persistService.removeConfigHistory(startTime, pageSize);
              removeTime--;
            }
          }
        } catch (Throwable e) {
          log.error("clearConfigHistory error", e);
        }
      }
    }
  };

  try {
    dumpConfigInfo(dumpAllProcessor);

    // 更新beta缓存
    LogUtil.defaultLog.info("start clear all config-info-beta.");
    DiskUtil.clearAllBeta();
    if (persistService.isExistTable(BETA_TABLE_NAME)) {
      dumpAllBetaProcessor.process(DumpAllBetaTask.TASK_ID, new DumpAllBetaTask());
    }
    // 更新Tag缓存
    LogUtil.defaultLog.info("start clear all config-info-tag.");
    DiskUtil.clearAllTag();
    if (persistService.isExistTable(TAG_TABLE_NAME)) {
      dumpAllTagProcessor.process(DumpAllTagTask.TASK_ID, new DumpAllTagTask());
    }

    // add to dump aggr
    List<ConfigInfoChanged> configList = persistService.findAllAggrGroup();
    if (configList != null && !configList.isEmpty()) {
      total = configList.size();
      List<List<ConfigInfoChanged>> splitList = splitList(configList, INIT_THREAD_COUNT);
      for (List<ConfigInfoChanged> list : splitList) {
        MergeAllDataWorker work = new MergeAllDataWorker(list);
        work.start();
      }
      log.info("server start, schedule merge end.");
    }
  } catch (Exception e) {
    LogUtil.fatalLog.error(
      "Nacos Server did not start because dumpservice bean construction failure :\n" + e.getMessage(),
      e.getCause());
    throw new RuntimeException(
      "Nacos Server did not start because dumpservice bean construction failure :\n" + e.getMessage());
  }
  if (!STANDALONE_MODE) {
    Runnable heartbeat = new Runnable() {
      @Override
      public void run() {
        String heartBeatTime = TimeUtils.getCurrentTime().toString();
        // write disk
        try {
          DiskUtil.saveHeartBeatToDisk(heartBeatTime);
        } catch (IOException e) {
          LogUtil.fatalLog.error("save heartbeat fail" + e.getMessage());
        }
      }
    };

    TimerTaskService.scheduleWithFixedDelay(heartbeat, 0, 10, TimeUnit.SECONDS);

    long initialDelay = new Random().nextInt(INITIAL_DELAY_IN_MINUTE) + 10;
    LogUtil.defaultLog.warn("initialDelay:{}", initialDelay);

    TimerTaskService.scheduleWithFixedDelay(dumpAll, initialDelay, DUMP_ALL_INTERVAL_IN_MINUTE,
                                            TimeUnit.MINUTES);

    TimerTaskService.scheduleWithFixedDelay(dumpAllBeta, initialDelay, DUMP_ALL_INTERVAL_IN_MINUTE,
                                            TimeUnit.MINUTES);
  }

  TimerTaskService.scheduleWithFixedDelay(clearConfigHistory, 10, 10, TimeUnit.MINUTES);

}
```



## 简单总结

简单总结一下刚刚的过程

- 客户端发起长轮询
- 服务端收到请求以后，先比较服务端缓存中的数据是否相同，如果不同直接返回
- 如果相同，则通过定时任务延迟29.5s在执行比较
- 为了保证当服务端在29.5s之内发生变化能够及时通知给客户端，服务端采用事件订阅的方式来监听服务端本地数据变化的事件，一旦接受到事件，则触发DataChangeTask的通知，并且便利allStubs队列中的ClientLongPolling，把结果协会到客户端，就完成一次数据推送。
- 如果DataChangeTask任务完成数据推送之后，ClientLongPolling中的调度任务又开始执行了怎么办呢？只要在进行推行操作之前，现在原来等待执行的调度任务取消掉就可以了，这样就防止了推送操作写完响应数据之后，调度任务又去写响应数据，这时肯定会报错。

所以总的来说，nacos采用push+pull的形式，来解决最开始关于长轮询时间间隔的问题。





# 集群选举源码分析

nacos 可以集群部署，就涉及到主从，nacos是通过什么机制来实现集群的呢？

## 选举算法

Nacos 集群采用raft算法来实现，它是相对zk的选举算法较为简单的一种。

在Raft中，节点有三种角色：

- leader：负责接收客户端的请求
- Candidate：用于选举leader的一种角色
- Follower：负责响应来着leader或者Candidate的请求

选举分为两个节点

- 服务启动的时候
- leader 不可使用的时候

所有节点启动单额时候，都是follower状态，如果在一段时间内没有收到leader的心跳，那么follower会变成candidate。然后发起选举，选举之前会增加term（这个term和zk中的epoch的道理是一样的）

- follower 会投自己一票，并且给其他节点发生票据vote，等到其他节点回复
- 在这个过程中，可能出现的几种情况
  - 收到过半的票数通过成为leader
  - 被告知其他节点以及成为leader，则自己切换成follower
  - 一段时间内没有收到过半的投票，则重新发起选举
  - 约束条件在任意term中，单个节点最多只能投一票



选举的几种情况

1. 赢得选举之后，leader会给所有节点发送消息，避免其他节点触发新的选举
2. 比如有三个几点ABC，AB同时发起选举，而C先接收到A的票据，选举A为leader；当B的票据来的时候，不满足只能投票一次的约束，AB也不会给对方投票，所有A胜出成为leader，并给BC发送心跳消息，节点B发现节点A的term不低于自己的term，就切换成follower
3. 可能出现平票的情况。假如有4个节点ABCD，AB成为了candidate，节点c投了a，节点d投了b，出现平票的情况，直到超时从新发起选举。如果出现平票的情况，那么久延长了系统不可用的时间，因此raft引入了randomized election timeouts来尽量避免平票的情况。



## 数据的处理

对于事务操作，请求会转发到leader，非事务操作，可以任意一个节点来处理



### RaftCore.signalPublish()

```java
public void signalPublish(String key, Record value) throws Exception {
	// 判断当前节点是否是leader
  if (!isLeader()) {
    JSONObject params = new JSONObject();
    params.put("key", key);
    params.put("value", value);
    Map<String, String> parameters = new HashMap<>(1);
    parameters.put("key", key);
		// 转发给leader处理
    raftProxy.proxyPostLarge(getLeader().ip, API_PUB, params.toJSONString(), parameters);
    return;
  }

  try {
    OPERATE_LOCK.lock();
    long start = System.currentTimeMillis();
    final Datum datum = new Datum();
    datum.key = key;
    datum.value = value;
    if (getDatum(key) == null) {
      datum.timestamp.set(1L);
    } else {
      datum.timestamp.set(getDatum(key).timestamp.incrementAndGet());
    }

    JSONObject json = new JSONObject();
    json.put("datum", datum);
    json.put("source", peers.local());
		// 向所有节点发送
    onPublish(datum, peers.local());

    final String content = JSON.toJSONString(json);

    final CountDownLatch latch = new CountDownLatch(peers.majorityCount());
    for (final String server : peers.allServersIncludeMyself()) {
      if (isLeader(server)) {
        latch.countDown();
        continue;
      }
      final String url = buildURL(server, API_ON_PUB);
      HttpClient.asyncHttpPostLarge(url, Arrays.asList("key=" + key), content, new AsyncCompletionHandler<Integer>() {
        @Override
        public Integer onCompleted(Response response) throws Exception {
          if (response.getStatusCode() != HttpURLConnection.HTTP_OK) {
            Loggers.RAFT.warn("[RAFT] failed to publish data to peer, datumId={}, peer={}, http code={}",
                              datum.key, server, response.getStatusCode());
            return 1;
          }
          latch.countDown();
          return 0;
        }

        @Override
        public STATE onContentWriteCompleted() {
          return STATE.CONTINUE;
        }
      });

    }

    if (!latch.await(UtilsAndCommons.RAFT_PUBLISH_TIMEOUT, TimeUnit.MILLISECONDS)) {
      // only majority servers return success can we consider this update success
      Loggers.RAFT.error("data publish failed, caused failed to notify majority, key={}", key);
      throw new IllegalStateException("data publish failed, caused failed to notify majority, key=" + key);
    }

    long end = System.currentTimeMillis();
    Loggers.RAFT.info("signalPublish cost {} ms, key: {}", (end - start), key);
  } finally {
    OPERATE_LOCK.unlock();
  }
}

```











