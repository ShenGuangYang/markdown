# kafka 基本使用

## 客户端使用

### pom.xml

```xml
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>2.0.0</version>
</dependency>
```



### producer 

```java

public class Producer extends Thread {

    private final KafkaProducer<Integer, String> kafkaProducer;
    private final String topic;

    public Producer(String topic) {
        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                IntegerSerializer.class.getName());
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class.getName());
        properties.put(ProducerConfig.CLIENT_ID_CONFIG, "producer"); // client_id 设置
        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, "16kb");//批量提交的数量
        properties.put(ProducerConfig.LINGER_MS_CONFIG, "5ms"); //批量提交的时间间隔
        kafkaProducer = new KafkaProducer<>(properties);
        this.topic = topic;
    }

    @Override
    public void run() {
        for (int i = 0; i < 1000; i++) {
            String msg = "pratice test message: " + i;
            try {
                RecordMetadata data = kafkaProducer.send(new ProducerRecord<>(topic, msg)).get();
                System.out.println(data.offset() + " -> " + data.topic() + "->" + data.partition());
                TimeUnit.SECONDS.sleep(1);
            } catch (InterruptedException e) {
                e.printStackTrace();
            } catch (ExecutionException e) {
                e.printStackTrace();
            }
        }
    }

    public static void main(String[] args) {
        new Producer("test").start();
    }

}

```



### consumer

```java
public class Consumer extends Thread{

    private  KafkaConsumer<Integer, String> kafkaConsumer;
    private String topic;

    public Consumer(String topic)   {
        Properties properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "consumer");

        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); // 设置自动提交
        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000);  // 自动提交的频率

        properties.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "30000");
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // 消费机制
        kafkaConsumer = new KafkaConsumer<>(properties);
        this.topic = topic;
    }

    @Override
    public void run() {
        while (true) {
            kafkaConsumer.subscribe(Collections.singleton("test"));
            ConsumerRecords<Integer, String> records = kafkaConsumer.poll(Duration.ofSeconds(1));
            records.forEach((record) -> {
                System.out.println(record.offset() + "->"+ record.partition() + "->" + record.key()
                        + "->" + record.value());
            });
        }
    }

    public static void main(String[] args) {
        new Consumer("test").start();
    }
}
```



### 发送端自定义分区策略，消费端自定义使用

#### 自定义分区策略

```java
public class MyPartitioner implements Partitioner {
    private Random random = new Random();

    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {

        List<PartitionInfo> partitionInfos = cluster.partitionsForTopic(topic);
        Integer countForTopic = cluster.partitionCountForTopic(topic);
        int partitionNum = 0;

        if (key == null) {
            partitionNum = random.nextInt(countForTopic);
        } else {
            partitionNum = Math.abs(key.hashCode()) % countForTopic;
        }
        return partitionNum;
    }

    @Override
    public void close() {

    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}
```

#### 发送端新增配置

```java
properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());
```

#### 消费端指定消费分区

```java
TopicPartition topicPartition=new TopicPartition(topic, 0); // 指定分区
kafkaConsumer.assign(Arrays.asList(topicPartition));
```

## springboot使用

jar导入，可能有版本依赖问题 [依赖关系](https://spring.io/projects/spring-kafka)



#### pom

```xml
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-parent</artifactId>
  <version>2.3.0.RELEASE</version>
</dependency>
<dependency>
  <groupId>org.springframework.kafka</groupId>
  <artifactId>spring-kafka</artifactId>
  <version>2.5.0.RELEASE</version>
</dependency>
```



#### 配置文件

```properties
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.IntegerSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

spring.kafka.consumer.group-id=consumer
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true

spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.IntegerDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
```



#### Consumer

```java
@Component
public class Consumer {

    @KafkaListener(topics = {"test"})
    public void listener(ConsumerRecord record) {
        Optional<?> msg = Optional.ofNullable(record.value());
        if (msg.isPresent()) {
            System.out.println(msg.get());
        }
    }
}
```

#### Producer

```java
@Component
public class Producer {
    @Autowired
    private KafkaTemplate<Integer, String> template;
    public void send() {
        template.send("test", 1, "msg");
    }
}
```

#### 测试

```java
@SpringBootApplication
public class Application {
    public static void main(String[] args) throws InterruptedException {
        ConfigurableApplicationContext context = SpringApplication.run(Application.class, args);
        Producer producer = context.getBean(Producer.class);
        for (int i = 0; i < 10; i++) {
            producer.send();
            TimeUnit.MILLISECONDS.sleep(100);
        }
    }
}
```

